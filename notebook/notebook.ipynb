{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../modules/environment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../modules/environment.py\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, env_size: int, n_enemy: int = 1) -> None:\n",
    "        self.env_size = env_size\n",
    "        self.n_enemy = n_enemy\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.food = np.random.randint(self.env_size, size=2)\n",
    "        self.enemies = [np.random.randint(\n",
    "            self.env_size, size=2) for _ in range(self.n_enemy)]\n",
    "        while True:\n",
    "            self.player = np.random.randint(\n",
    "                self.env_size, size=2)  # need to fix\n",
    "            self.get_reward_and_done()  # update self._done\n",
    "            if not self._done:  # self._done == False\n",
    "                break\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        def make_string(xy): return str(xy[0])+\"_\"+str(xy[1])\n",
    "        order_enem = sorted([tuple(coordinate) for coordinate in self.enemies])\n",
    "        return \"-\".join(\n",
    "            list(\n",
    "                map(\n",
    "                    make_string,\n",
    "                    [self.player, self.food, *order_enem]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "        # 1 : player\n",
    "        # 2 : food\n",
    "        # 3 : enemy\n",
    "        p = tuple(self.player)\n",
    "        f = tuple(self.food)\n",
    "        grid[p] = 1\n",
    "        grid[f] = 2\n",
    "        if f == p:\n",
    "            grid[f] = 5\n",
    "        for enemy in self.enemies:\n",
    "            e = tuple(enemy)\n",
    "            grid[e] = 3\n",
    "            if f == e:\n",
    "                grid[f] = 4\n",
    "            if e == p:\n",
    "                grid[e] = 6\n",
    "            if e == f and f == p:\n",
    "                grid[e] = 7\n",
    "\n",
    "        return grid\n",
    "\n",
    "    def __repr__(self):\n",
    "        grid = self.render()\n",
    "        repr = {0: \" \", 1: \"P\", 2: \"F\", 3: \"E\", 4: \"?\", 5: \"@\", 6: \"X\", 7: \"#\"}\n",
    "        result = \" \" + \"-\"*self.env_size + \"\\n\"\n",
    "        for row in grid:\n",
    "            row_string = \"|\"\n",
    "            for col in row:\n",
    "                row_string += repr[col]\n",
    "            row_string += \"|\\n\"\n",
    "            result += row_string\n",
    "        result += \" \" + \"-\"*self.env_size\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def done(self):\n",
    "        return self._done\n",
    "\n",
    "    def step(self, palyer_direction):\n",
    "        # move player\n",
    "        self.move(\"player\", palyer_direction)\n",
    "\n",
    "        # always move enemies randomly\n",
    "        for i in range(self.n_enemy):\n",
    "            enemy_direction = np.random.randint(1, 5)\n",
    "            self.move(f\"enemy{i}\", enemy_direction)\n",
    "\n",
    "        # get reward after a step\n",
    "        reward, done = self.get_reward_and_done()\n",
    "        next_state = self.state\n",
    "        return reward, next_state, done\n",
    "\n",
    "    def move(self, entity_name, direction):\n",
    "        if entity_name == \"player\":\n",
    "            entity = self.player\n",
    "        elif \"enemy\" in entity_name:\n",
    "            e_index = int(entity_name[5:])\n",
    "            entity = self.enemies[e_index]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid entity_name {entity_name}\")\n",
    "\n",
    "        if direction == 0:  # do not move\n",
    "            pass\n",
    "        elif direction == 1:  # left\n",
    "            # not allowed to go out of the wall\n",
    "            entity[1] = max(0, entity[1] - 1)\n",
    "        elif direction == 2:  # right\n",
    "            # not allowed to go out of the wall\n",
    "            entity[1] = min(self.env_size - 1, entity[1] + 1)\n",
    "        elif direction == 3:  # up\n",
    "            # not allowed to go out of the wall\n",
    "            entity[0] = max(0, entity[0] - 1)\n",
    "        elif direction == 4:  # down\n",
    "            # not allowed to go out of the wall\n",
    "            entity[0] = min(self.env_size - 1, entity[0] + 1)\n",
    "\n",
    "    def get_reward_and_done(self):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if np.all(self.player == self.food):\n",
    "            reward += 1\n",
    "            done = True\n",
    "        for enemy in self.enemies:\n",
    "            if np.all(self.player == enemy):\n",
    "                reward -= 1\n",
    "                done = True\n",
    "        self._done = done\n",
    "        return reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../modules/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../modules/utils.py\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "def save_to_json(data, path):\n",
    "    with open(path, \"w\") as json_file:\n",
    "        dumped = {state: qvalue.tolist() for state, qvalue in data.items()}\n",
    "        json.dump(dumped, json_file, indent=3)\n",
    "\n",
    "\n",
    "def read_from_json(path):\n",
    "    with open(path, \"r\") as json_file:\n",
    "        content = json.load(json_file)\n",
    "    data = {state: np.array(qvalue) for state, qvalue in content.items()}\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../modules/agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../modules/agent.py\n",
    "from modules.utils import save_to_json, read_from_json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, num_actions, gamma, learning_rate, epsilon, qfunction=None) -> None:\n",
    "        self.n_actions = num_actions\n",
    "        self.qfunction = {} if qfunction is None else qfunction if isinstance(\n",
    "            qfunction, dict) else read_from_json(qfunction)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self, state, eval=False):\n",
    "        if state not in self.qfunction:\n",
    "            self.qfunction[state] = np.zeros(self.n_actions)\n",
    "        u = np.random.uniform()\n",
    "        if eval or self.epsilon < u:\n",
    "            action = np.argmax(self.qfunction[state])\n",
    "        else:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if state not in self.qfunction:\n",
    "            self.qfunction[state] = np.zeros(self.n_actions)\n",
    "\n",
    "        if done:\n",
    "            self.qfunction[state][action] = (\n",
    "                1 - self.alpha)*self.qfunction[state][action] + self.alpha*reward\n",
    "        else:\n",
    "            if next_state not in self.qfunction:\n",
    "                self.qfunction[next_state] = np.zeros(self.n_actions)\n",
    "            self.qfunction[state][action] = (1 - self.alpha)*self.qfunction[state][action] + \\\n",
    "                self.alpha*(reward + self.gamma *\n",
    "                            self.qfunction[next_state].max())\n",
    "\n",
    "    def save_qfunction(self, path=\"./qfunction.json\"):\n",
    "        save_to_json(self.qfunction, path)\n",
    "\n",
    "    def load_qfunction(self, path=\"./qfunction.json\"):\n",
    "        self.qfunction = read_from_json(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../modules/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../modules/train.py\n",
    "from modules.environment import Environment\n",
    "from modules.agent import QAgent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_episode(env: Environment, agent: QAgent, max_step: int, eval: bool, show=False):\n",
    "    state = env.reset()\n",
    "    actions_name = {0: \"halt\", 1: \"left\", 2: \"right\", 3: \"up\", 4: \"down\"}\n",
    "    n_step = 1\n",
    "    if show:\n",
    "        print(\n",
    "            \"Symbols meaning [P: Player/Agent, E: enemy, F: food/goal, ?: E+F, @: P+F, X: P+E, #: P+E+F]\")\n",
    "        print(\"New Environment :\")\n",
    "        print(env)\n",
    "    while True:\n",
    "        action = agent.act(state, eval)\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if show:\n",
    "            print(\n",
    "                f\"action : {actions_name[action]} is taken at step: {n_step}\\n\")\n",
    "            print(env)\n",
    "        if not eval:\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        n_step += 1\n",
    "        if done or n_step > max_step:\n",
    "            break\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def train(environment: Environment, agent: QAgent, num_episode: int, max_step: int = 1000):\n",
    "    rewards = []\n",
    "    episodes = []\n",
    "    eval_every = 10\n",
    "    n_eval = 5\n",
    "    try:\n",
    "        for ep in tqdm(range(1, num_episode+1)):\n",
    "            run_episode(environment, agent, max_step, eval=False, show=False)\n",
    "            if ep % 10 == 0:\n",
    "                episodes.append(ep)\n",
    "                mean_reward = np.mean([run_episode(\n",
    "                    environment, agent, max_step, eval=True, show=False) for _ in range(n_eval)])\n",
    "                rewards.append(mean_reward)\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Training interrupted at {ep} episodes.\")\n",
    "        input(\"Press enter to continue ...\")\n",
    "\n",
    "    return episodes, rewards\n",
    "\n",
    "\n",
    "def plot_rewards(episodes, rewards):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Average reward during training.\")\n",
    "    plt.plot(episodes, rewards, \"g:\", label=\"average reward\")\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(\"reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../generate-qfunction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../generate-qfunction.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from modules.environment import Environment\n",
    "from modules.agent import QAgent\n",
    "from modules.train import train, plot_rewards, run_episode\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(\n",
    "    description=\"Generate a qvalue function in a json file.\")\n",
    "\n",
    "parser.add_argument(\"-n\", \"--env-size\", type=int,\n",
    "                    help=\"size of the environment\")\n",
    "parser.add_argument(\"-en\", \"--n-enem\", type=int,\n",
    "                    help=\"number of enemy in the evironment\")\n",
    "parser.add_argument(\"-g\", \"--gamma\", type=float,\n",
    "                    help=\"discount factor (horizon-size)\")\n",
    "parser.add_argument(\"-lr\", \"--learning-rate\", type=float,\n",
    "                    help=\"learning rate/alpha (exploitation)\")\n",
    "parser.add_argument(\"-eps\", \"--epsilon\", type=float,\n",
    "                    help=\"probability to act non-greedy (exploration)\")\n",
    "parser.add_argument(\"-epi\", \"--n-episode\", type=int,\n",
    "                    help=\"number of episodes for training\")\n",
    "parser.add_argument(\"-max\", \"--max-step\", type=int,\n",
    "                    help=\"maximum number of steps for each episode\")\n",
    "parser.add_argument(\"-l\", \"--path-to-load-qfunction\", type=str,\n",
    "                    help=\"path for the json file of qvalue function to load\")\n",
    "parser.add_argument(\"-s\", \"--path-to-save-qfunction\", type=str,\n",
    "                    help=\"path for the json file of qvalue function to save\")\n",
    "parser.add_argument(\"-show\", \"--show\", type=bool,\n",
    "                    help=\"flag either user want to show the reward during training\")\n",
    "\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "env_size = args[\"env_size\"]\n",
    "n_enem = args.get(\"n_enem\",1)\n",
    "num_actions = 5\n",
    "gamma = args[\"gamma\"]\n",
    "learning_rate = args[\"learning_rate\"]\n",
    "epsilon = args[\"epsilon\"]\n",
    "n_episode = args[\"n_episode\"]\n",
    "max_step = args[\"max_step\"]\n",
    "path_load_qfunction = args.get(\"path_to_load_qfunction\", None)\n",
    "path_save_qfunction = args.get(\n",
    "    \"path_to_save_qfunction\", None)\n",
    "show_plot = args.get(\"show\", False)\n",
    "\n",
    "\n",
    "env = Environment(env_size=env_size, n_enemy=n_enem)\n",
    "agent = QAgent(num_actions=num_actions, gamma=gamma,\n",
    "               learning_rate=learning_rate, epsilon=epsilon, qfunction=path_load_qfunction)\n",
    "print(\"Start trainining...\")\n",
    "episodes, rewards = train(environment=env, agent=agent,\n",
    "                          num_episode=n_episode, max_step=max_step)\n",
    "print(\"Training done successfully.\")\n",
    "if path_save_qfunction is not None:\n",
    "    agent.save_qfunction(path_save_qfunction)\n",
    "if show_plot:\n",
    "    plot_rewards(episodes, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.environment import Environment\n",
    "from modules.agent import QAgent\n",
    "from modules.train import train, plot_rewards, run_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_size = 5\n",
    "n_enem = 3\n",
    "num_actions = 5\n",
    "gamma = 0.999\n",
    "learning_rate = 0.01\n",
    "epsilon = 1\n",
    "n_episode = 5000000\n",
    "max_step = 1000\n",
    "path_load_qfunction = None\n",
    "path_save_qfunction = f\"../qfunction/qfunction-size{env_size}_enem{n_enem}.json\"\n",
    "show_plot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18189/5000000 [00:10<46:22, 1790.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted at 18190 episodes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1819,) and (1818,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x4/45r2t3bx5l3dc_b3tm_yz7380000gn/T/ipykernel_84730/790132560.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_qfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_save_qfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplot_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Codes/maze-env-player-food-enemy/modules/train.py\u001b[0m in \u001b[0;36mplot_rewards\u001b[0;34m(episodes, rewards)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average reward during training.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"average reward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2769\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2770\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1819,) and (1818,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Environment(env_size=env_size, n_enemy=n_enem)\n",
    "agent = QAgent(num_actions=num_actions, gamma=gamma,\n",
    "               learning_rate=learning_rate, epsilon=epsilon, qfunction=path_load_qfunction)\n",
    "\n",
    "print(\"Start trainining...\")\n",
    "episodes, rewards = train(environment=env, agent=agent,\n",
    "                          num_episode=n_episode, max_step=max_step)\n",
    "print(\"Training done successfully.\")\n",
    "\n",
    "agent.save_qfunction(path_save_qfunction)\n",
    "if show_plot:\n",
    "    plot_rewards(episodes, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols [P: Player/Agent, E: enemy, F: food/goal, ?: E+F, @: P+F, X: P+E, #: P+E+F]\n",
      "New Environment :\n",
      " -----\n",
      "|     |\n",
      "|     |\n",
      "|     |\n",
      "|   F |\n",
      "|P  E |\n",
      " -----\n",
      "action : up is taken at step: 1\n",
      "\n",
      " -----\n",
      "|     |\n",
      "|     |\n",
      "|     |\n",
      "|P  F |\n",
      "|    E|\n",
      " -----\n",
      "action : right is taken at step: 2\n",
      "\n",
      " -----\n",
      "|     |\n",
      "|     |\n",
      "|     |\n",
      "| P F |\n",
      "|   E |\n",
      " -----\n",
      "action : right is taken at step: 3\n",
      "\n",
      " -----\n",
      "|     |\n",
      "|     |\n",
      "|     |\n",
      "|  PF |\n",
      "|    E|\n",
      " -----\n",
      "action : right is taken at step: 4\n",
      "\n",
      " -----\n",
      "|     |\n",
      "|     |\n",
      "|     |\n",
      "|   @ |\n",
      "|    E|\n",
      " -----\n",
      "Final reward : 1\n"
     ]
    }
   ],
   "source": [
    "reward = run_episode(env,agent, max_step, eval=True, show=True)\n",
    "print(f\"Final reward : {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
