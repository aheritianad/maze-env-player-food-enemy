{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile environment.py\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, env_size) -> None:\n",
    "        self.env_size = env_size\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.food = np.random.randint(self.env_size, size=2) \n",
    "        self.enemy = np.random.randint(self.env_size, size=2)\n",
    "        while True:\n",
    "            self.player = np.random.randint(self.env_size, size=2) # need to fix\n",
    "            self.get_reward_and_done()\n",
    "            if not self._done: # self._done == False  \n",
    "                break\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    @property\n",
    "    def state(self):\n",
    "        return \"-\".join(\n",
    "                        list(\n",
    "                            map(\n",
    "                                lambda xy:str(xy[0])+\"_\"+str(xy[1]),\n",
    "                                [self.player,self.food,self.enemy]\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "        # 1 : player\n",
    "        # 2 : food\n",
    "        # 3 : enemy\n",
    "        p = tuple(self.player)\n",
    "        f = tuple(self.food)\n",
    "        e = tuple(self.enemy)\n",
    "        grid[p] = 1\n",
    "        grid[f] = 2\n",
    "        grid[e] = 3\n",
    "        if f == e:\n",
    "            grid[f] = 4\n",
    "        if f == p:\n",
    "            grid[f] = 5\n",
    "        if e == p:\n",
    "            grid[e] = 6\n",
    "        if e == f and f == p:\n",
    "            grid[e] = 7\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def __repr__(self):\n",
    "        grid = self.render()\n",
    "        repr = {0:\" \", 1:\"P\", 2:\"F\", 3:\"E\", 4:\"?\", 5:\"@\", 6:\"X\", 7:\"#\"}\n",
    "        result = \" \" + \"-\"*self.env_size + \"\\n\"\n",
    "        for row in grid:\n",
    "            row_string = \"|\"\n",
    "            for col in row:\n",
    "                row_string += repr[col]\n",
    "            row_string += \"|\\n\"\n",
    "            result += row_string\n",
    "        result +=  \" \" + \"-\"*self.env_size\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def done(self):\n",
    "        return self._done\n",
    "            \n",
    "    def step(self, palyer_direction):\n",
    "        # move player\n",
    "        self.move(\"player\",palyer_direction)\n",
    "\n",
    "        # always move enemy randomly\n",
    "        enemy_direction = np.random.randint(1,5)\n",
    "        self.move(\"enemy\",enemy_direction)\n",
    "\n",
    "        # get reward after a step\n",
    "        reward, done = self.get_reward_and_done()\n",
    "        next_state = self.state\n",
    "        return reward, next_state, done\n",
    "\n",
    "    def move(self, entity_name, direction):\n",
    "        entities = {\"player\":self.player, \"enemy\":self.enemy}\n",
    "        entity = entities[entity_name]\n",
    "\n",
    "        if direction == 0: # do not move\n",
    "            pass\n",
    "        elif direction == 1: # left\n",
    "            entity[1] = max(0, entity[1] - 1 ) # not allowed to go out of the wall\n",
    "        elif direction == 2: # right\n",
    "            entity[1] = min(self.env_size - 1, entity[1] + 1 ) # not allowed to go out of the wall\n",
    "        elif direction == 3: # up\n",
    "            entity[0] = max(0, entity[0] - 1 ) # not allowed to go out of the wall\n",
    "        elif direction == 4: # down\n",
    "            entity[0] = min(self.env_size - 1, entity[0] + 1 ) # not allowed to go out of the wall\n",
    "    \n",
    "    def get_reward_and_done(self):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if np.all(self.player == self.food):\n",
    "            reward += 1\n",
    "            done = True\n",
    "        if np.all(self.player == self.enemy):\n",
    "            reward -= 1\n",
    "            done = True\n",
    "        self._done = done\n",
    "        return reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "def save_to_json(data, path):\n",
    "    with open(path, \"w\") as json_file:\n",
    "        dumped = {state: qvalue.tolist() for state, qvalue in data.items()}\n",
    "        json.dump(dumped, json_file, indent=3)\n",
    "\n",
    "\n",
    "def read_from_json(path):\n",
    "    with open(path, \"r\") as json_file:\n",
    "        content = json.load(json_file)\n",
    "    data = {state: np.array(qvalue) for state, qvalue in content.items()}\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from utils import save_to_json, read_from_json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, num_actions, gamma, learning_rate, epsilon, qfunction=None) -> None:\n",
    "        self.n_actions = num_actions\n",
    "        self.qfunction = {} if qfunction is None else qfunction if isinstance(\n",
    "            qfunction, dict) else read_from_json(qfunction)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self, state, eval=False):\n",
    "        if state not in self.qfunction:\n",
    "            self.qfunction[state] = np.zeros(self.n_actions)\n",
    "        u = np.random.uniform()\n",
    "        if eval or self.epsilon < u:\n",
    "            action = np.argmax(self.qfunction[state])\n",
    "        else:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if state not in self.qfunction:\n",
    "            self.qfunction[state] = np.zeros(self.n_actions)\n",
    "\n",
    "        if done:\n",
    "            self.qfunction[state][action] = (\n",
    "                1 - self.alpha)*self.qfunction[state][action] + self.alpha*reward\n",
    "        else:\n",
    "            if next_state not in self.qfunction:\n",
    "                self.qfunction[next_state] = np.zeros(self.n_actions)\n",
    "            self.qfunction[state][action] = (1 - self.alpha)*self.qfunction[state][action] + \\\n",
    "                self.alpha*(reward + self.gamma *\n",
    "                            self.qfunction[next_state].max())\n",
    "\n",
    "    def save_qfunction(self, path=\"./qfunction.json\"):\n",
    "        save_to_json(self.qfunction, path)\n",
    "\n",
    "    def load_qfunction(self, path=\"./qfunction.json\"):\n",
    "        self.qfunction = read_from_json(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "from environment import Environment\n",
    "from agent import QAgent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_episode(env: Environment, agent: QAgent, max_step: int, eval: bool, show=False):\n",
    "    state = env.reset()\n",
    "    actions_name = {0: \"halt\", 1: \"left\", 2: \"right\", 3: \"up\", 4: \"down\"}\n",
    "    n_step = 1\n",
    "    while True:\n",
    "        action = agent.act(state, eval)\n",
    "        if show:\n",
    "            print(env)\n",
    "            print(f\"step: {n_step}\\t action : {actions_name[action]}\\n\")\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if not eval:\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        n_step += 1\n",
    "        if done or n_step > max_step:\n",
    "            break\n",
    "    if show:\n",
    "        print(env)\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def train(environment: Environment, agent: QAgent, num_episode: int, max_step: int = 1000):\n",
    "    rewards = []\n",
    "    episodes = []\n",
    "    eval_every = 10\n",
    "    n_eval = 5\n",
    "    for ep in tqdm(range(1, num_episode+1)):\n",
    "        run_episode(environment, agent, max_step, eval=False, show=False)\n",
    "        if ep % 10 == 0:\n",
    "            episodes.append(ep)\n",
    "            mean_reward = np.mean([run_episode(\n",
    "                environment, agent, max_step, eval=True, show=False) for _ in range(n_eval)])\n",
    "            rewards.append(mean_reward)\n",
    "\n",
    "    return episodes, rewards\n",
    "\n",
    "\n",
    "def plot_rewards(episodes, rewards):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Average reward during training.\")\n",
    "    plt.plot(episodes, rewards, \"g:\", label=\"average reward\")\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(\"reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile generate-qfunction.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from environment import Environment\n",
    "from agent import QAgent\n",
    "from train import train, plot_rewards, run_episode\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(\n",
    "    description=\"Generate a qvalue function in a json file.\")\n",
    "\n",
    "parser.add_argument(\"-n\", \"--env-size\", type=int,\n",
    "                    help=\"size of the environment\")\n",
    "parser.add_argument(\"-g\", \"--gamma\", type=float,\n",
    "                    help=\"discount factor (horizon-size)\")\n",
    "parser.add_argument(\"-lr\", \"--learning-rate\", type=float,\n",
    "                    help=\"learning rate/alpha (exploitation)\")\n",
    "parser.add_argument(\"-eps\", \"--epsilon\", type=float,\n",
    "                    help=\"probability to act non-greedy (exploration)\")\n",
    "parser.add_argument(\"-epi\", \"--n-episode\", type=int,\n",
    "                    help=\"number of episodes for training\")\n",
    "parser.add_argument(\"-max\", \"--max-step\", type=int,\n",
    "                    help=\"maximum number of steps for each episode\")\n",
    "parser.add_argument(\"-l\", \"--path-to-load-qfunction\", type=str,\n",
    "                    help=\"path for the json file of qvalue function to load\")\n",
    "parser.add_argument(\"-s\", \"--path-to-save-qfunction\", type=str,\n",
    "                    help=\"path for the json file of qvalue function to save\")\n",
    "parser.add_argument(\"-show\", \"--show\", type=bool,\n",
    "                    help=\"flag either user want to show the reward during training\")\n",
    "\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "env_size = args[\"env_size\"]\n",
    "num_actions = 5\n",
    "gamma = args[\"gamma\"]\n",
    "learning_rate = args[\"learning_rate\"]\n",
    "epsilon = args[\"epsilon\"]\n",
    "n_episode = args[\"n_episode\"]\n",
    "max_step = args[\"max_step\"]\n",
    "path_load_qfunction = args.get(\"path_to_load_qfunction\", None)\n",
    "path_save_qfunction = args.get(\n",
    "    \"path_to_save_qfunction\", f\"./qfunction-{env_size}.json\")\n",
    "show_plot = args.get(\"show\", False)\n",
    "\n",
    "\n",
    "env = Environment(env_size=env_size)\n",
    "agent = QAgent(num_actions=num_actions, gamma=gamma,\n",
    "               learning_rate=learning_rate, epsilon=epsilon, qfunction=path_load_qfunction)\n",
    "episodes, rewards = train(environment=env, agent=agent,\n",
    "                          num_episode=n_episode, max_step=max_step)\n",
    "agent.save_qfunction(path_save_qfunction)\n",
    "if show_plot:\n",
    "    plot_rewards(episodes, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
